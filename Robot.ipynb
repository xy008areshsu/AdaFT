{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "# import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt ('labelded_sss.csv', delimiter=\",\")\n",
    "data = pd.DataFrame(data, columns=['theta1', 'theta2', 'theta3', 'rate1', 'rate2', 'rate3', 'safe'])\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data.ix[:, ['theta1', 'theta2', 'theta3', 'rate1', 'rate2', 'rate3']].values\n",
    "y = data.ix[:, 'safe'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X = data.ix[:, ['theta1', 'theta2', 'rate1', 'rate2']].values\n",
    "# y = data.ix[:, 'safe'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.01)\n",
    "y_train[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stdsc = StandardScaler()\n",
    "X_train_norm = stdsc.fit_transform(X_train)\n",
    "X_test_norm = stdsc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clf = KNeighborsClassifier(n_neighbors = 10)\n",
    "# clf.fit(X_train_norm, y_train)\n",
    "# clf.score(X_train_norm, y_train), clf.score(X_test_norm, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators = 15, n_jobs = -1, class_weight='balanced')\n",
    "forest.fit(X_train, y_train)\n",
    "importances = forest.feature_importances_\n",
    "cols = data.columns\n",
    "importances = zip(cols, importances)\n",
    "for name, val in importances:\n",
    "    print(name, val)\n",
    "\n",
    "forest.score(X_train, y_train), forest.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores = forest.predict_proba(X_test)[:,1]\n",
    "precision, recall, threshold = precision_recall_curve(y_test, scores, pos_label=1)\n",
    "average_precision = average_precision_score(y_test, scores)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, label='area = %0.2f' % average_precision, color=\"green\")\n",
    "# plt.plot(threshold, precision[1:], label='area = %0.2f' % average_precision, color=\"red\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision Recall Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "len(precision), len(threshold), len(recall)\n",
    "\n",
    "plt.figure()\n",
    "# plt.plot(recall, precision, label='area = %0.2f' % average_precision, color=\"green\")\n",
    "plt.plot(threshold, precision[1:], label='area = %0.2f' % average_precision, color=\"red\")\n",
    "plt.grid(True)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('threshold')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Threshold Precision Curve')\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators = 100, n_jobs = -1)\n",
    "forest.fit(X_train_norm, y_train)\n",
    "importances = forest.feature_importances_\n",
    "cols = data.columns\n",
    "importances = zip(cols, importances)\n",
    "for name, val in importances:\n",
    "    print(name, val)\n",
    "\n",
    "forest.score(X_train_norm, y_train), forest.score(X_test_norm, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = AdaBoostClassifier(n_estimators=50)\n",
    "clf.fit(X_train_norm, y_train)\n",
    "importances = clf.feature_importances_\n",
    "cols = data.columns\n",
    "importances = zip(cols, importances)\n",
    "for name, val in importances:\n",
    "    print(name, val)\n",
    "\n",
    "clf.score(X_train_norm, y_train), clf.score(X_test_norm, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = data.ix[:, ['theta1', 'theta2', 'theta3', 'rate1', 'rate2', 'rate3']].values, data.ix[:, ['safe']].values\n",
    "# y = label_binarize(y, classes=[0, 1])\n",
    "# n_classes = y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stdsc = StandardScaler()\n",
    "X_train_norm = stdsc.fit_transform(X_train)\n",
    "X_test_norm = stdsc.transform(X_test)\n",
    "X_train_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = SVC(kernel='rbf', class_weight='balanced', C = 1., probability=True)\n",
    "clf.fit(X_train_norm, y_train)\n",
    "clf.score(X_test_norm, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rbf_feature = RBFSampler()\n",
    "X_train_features = rbf_feature.fit_transform(X_train)\n",
    "X_test_features = rbf_feature.transform(X_test)\n",
    "stdsc = StandardScaler()\n",
    "X_train_norm = stdsc.fit_transform(X_train)\n",
    "X_test_norm = stdsc.transform(X_test)\n",
    "X_train_norm.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = SGDClassifier(n_jobs = -1, loss= 'perceptron', class_weight= 'balanced')  \n",
    "clf.fit(X_train_norm, y_train)\n",
    "clf.score(X_test_norm, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = data.ix[:, ['theta1', 'theta2', 'theta3', 'rate1', 'rate2', 'rate3']].values, data.ix[:, ['safe']].values\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.float32)\n",
    "num_labels = 2\n",
    "# y = label_binarize(y, classes=[0, 1])\n",
    "# n_classes = y.shape[1]\n",
    "train_dataset, test_dataset, train_labels, test_labels = train_test_split(X, y, test_size=.1)\n",
    "valid_dataset, test_dataset, valid_labels, test_labels = train_test_split(test_dataset, test_labels, test_size=.5)\n",
    "train_labels = (np.arange(num_labels) == train_labels[:,None]).astype(np.float32)\n",
    "valid_labels = (np.arange(num_labels) == valid_labels[:,None]).astype(np.float32)\n",
    "test_labels = (np.arange(num_labels) == test_labels[:,None]).astype(np.float32)\n",
    "train_labels = train_labels.reshape((train_labels.shape[0], train_labels.shape[2]))\n",
    "valid_labels = valid_labels.reshape((valid_labels.shape[0], valid_labels.shape[2]))\n",
    "test_labels = test_labels.reshape((test_labels.shape[0], test_labels.shape[2]))\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_layer1_size = 1024\n",
    "hidden_layer2_size = 305\n",
    "hidden_lastlayer_size = 75\n",
    "data_size = 6\n",
    "num_labels = 2\n",
    "\n",
    "use_multilayers = True\n",
    "\n",
    "regularization_meta=0.03\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, data_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "  weights_layer1 = tf.Variable(\n",
    "    tf.truncated_normal([data_size, hidden_layer1_size], stddev=0.0517))\n",
    "  biases_layer1 = tf.Variable(tf.zeros([hidden_layer1_size]))\n",
    "\n",
    "  if use_multilayers:\n",
    "    weights_layer2 = tf.Variable(\n",
    "      tf.truncated_normal([hidden_layer1_size, hidden_layer1_size], stddev=0.0441))\n",
    "    biases_layer2 = tf.Variable(tf.zeros([hidden_layer1_size]))\n",
    "\n",
    "    weights_layer3 = tf.Variable(\n",
    "      tf.truncated_normal([hidden_layer1_size, hidden_layer2_size], stddev=0.0441))\n",
    "    biases_layer3 = tf.Variable(tf.zeros([hidden_layer2_size]))\n",
    "    \n",
    "    weights_layer4 = tf.Variable(\n",
    "      tf.truncated_normal([hidden_layer2_size, hidden_lastlayer_size], stddev=0.0809))\n",
    "    biases_layer4 = tf.Variable(tf.zeros([hidden_lastlayer_size]))\n",
    "\n",
    "\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([hidden_lastlayer_size if use_multilayers else hidden_layer1_size, num_labels], stddev=0.1632))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "    \n",
    "  # get the NN models\n",
    "  def getNN4Layer(dSet, use_dropout):\n",
    "    input_to_layer1 = tf.matmul(dSet, weights_layer1) + biases_layer1\n",
    "    hidden_layer1_output = tf.nn.relu(input_to_layer1)\n",
    "    \n",
    "    \n",
    "    logits_hidden1 = None\n",
    "    if use_dropout:\n",
    "       dropout_hidden1 = tf.nn.dropout(hidden_layer1_output, keep_prob)\n",
    "       logits_hidden1 = tf.matmul(dropout_hidden1, weights_layer2) + biases_layer2\n",
    "    else:\n",
    "      logits_hidden1 = tf.matmul(hidden_layer1_output, weights_layer2) + biases_layer2\n",
    "    \n",
    "    hidden_layer2_output = tf.nn.relu(logits_hidden1)\n",
    "    \n",
    "    logits_hidden2 = None\n",
    "    if use_dropout:\n",
    "       dropout_hidden2 = tf.nn.dropout(hidden_layer2_output, keep_prob)\n",
    "       logits_hidden2 = tf.matmul(dropout_hidden2, weights_layer3) + biases_layer3\n",
    "    else:\n",
    "      logits_hidden2 = tf.matmul(hidden_layer2_output, weights_layer3) + biases_layer3\n",
    "    \n",
    "    \n",
    "    hidden_layer3_output = tf.nn.relu(logits_hidden2)\n",
    "    logits_hidden3 = None\n",
    "    if use_dropout:\n",
    "       dropout_hidden3 = tf.nn.dropout(hidden_layer3_output, keep_prob)\n",
    "       logits_hidden3 = tf.matmul(dropout_hidden3, weights_layer4) + biases_layer4\n",
    "    else:\n",
    "      logits_hidden3 = tf.matmul(hidden_layer3_output, weights_layer4) + biases_layer4\n",
    "    \n",
    "    \n",
    "    hidden_layer4_output = tf.nn.relu(logits_hidden3)\n",
    "    logits = None\n",
    "    if use_dropout:\n",
    "       dropout_hidden4 = tf.nn.dropout(hidden_layer4_output, keep_prob)\n",
    "       logits = tf.matmul(dropout_hidden4, weights) + biases\n",
    "    else:\n",
    "      logits = tf.matmul(hidden_layer4_output, weights) + biases\n",
    "    \n",
    "    return logits\n",
    "\n",
    "  # get the NN models\n",
    "  def getNN1Layer(dSet, use_dropout, w1, b1, w, b):\n",
    "    input_to_layer1 = tf.matmul(dSet, w1) + b1\n",
    "    hidden_layer1_output = tf.nn.relu(input_to_layer1)\n",
    "        \n",
    "    logits = None\n",
    "    if use_dropout:\n",
    "       dropout_hidden1 = tf.nn.dropout(hidden_layer1_output, keep_prob)\n",
    "       logits = tf.matmul(dropout_hidden1, w) + b\n",
    "    else:\n",
    "      logits = tf.matmul(hidden_layer1_output, w) + b\n",
    "    \n",
    "    return logits\n",
    "\n",
    "  \n",
    "  \n",
    "  # Training computation.\n",
    "  logits = getNN4Layer(tf_train_dataset, True)  \n",
    "  logits_valid = getNN4Layer(tf_valid_dataset, False)\n",
    "  logits_test = getNN4Layer(tf_test_dataset, False)\n",
    "    \n",
    "  \n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  #loss_l2 = loss + (regularization_meta * (tf.nn.l2_loss(weights)))\n",
    "  \n",
    "  global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "  learning_rate = tf.train.exponential_decay(0.3, global_step, 3500, 0.86, staircase=True)\n",
    "  \n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(logits_valid)\n",
    "  test_prediction = tf.nn.softmax(logits_test)\n",
    "\n",
    "\n",
    "\n",
    "num_steps = 95001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in xrange(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    \n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    \n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, keep_prob:0.75}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step\", step, \":\", l)\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(train_prediction.eval(feed_dict={tf_train_dataset : batch_data, tf_train_labels : batch_labels, keep_prob:1.0}), batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(feed_dict={keep_prob:1.0}), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(feed_dict={keep_prob:1.0}), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1052838, 7)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.genfromtxt ('../AutomaticScriptForSubRegions/Robot/subspaces.csv', delimiter=\",\")\n",
    "data = pd.DataFrame(data, columns=['theta1', 'theta2', 'theta3', 'rate1', 'rate2', 'rate3', 'level'])\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# xtra_data = []\n",
    "# for theta1 in np.arange(-0.1, 0.2, 0.05):\n",
    "#     for theta2 in np.arange(-0.1, 0.2, 0.05):\n",
    "#         for theta3 in np.arange(-0.1, 0.2, 0.05):\n",
    "#             for rate1 in np.arange(-1, 2, 0.5):\n",
    "#                 for rate2 in np.arange(-1, 2, 0.5):\n",
    "#                     for rate3 in np.arange(-1, 2, 0.5):\n",
    "#                         xtra_data.append([theta1, theta2, theta3, rate1, rate2, rate3, 0])\n",
    "\n",
    "# data = np.concatenate((data, np.array(xtra_data)), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# np.savetxt(\"../AutomaticScriptForSubRegions/Robot/subspaces.csv\", data, delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (880875, 6), (880875, 3))\n",
      "('Validation set', (48937, 6), (48937, 3))\n",
      "('Test set', (48938, 6), (48938, 3))\n"
     ]
    }
   ],
   "source": [
    "X, y = data.ix[:, ['theta1', 'theta2', 'theta3', 'rate1', 'rate2', 'rate3']].values, data.ix[:, ['level']].values\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.float32)\n",
    "num_labels = 3\n",
    "# y = label_binarize(y, classes=[0, 1])\n",
    "# n_classes = y.shape[1]\n",
    "train_dataset, test_dataset, train_labels, test_labels = train_test_split(X, y, test_size=.1)\n",
    "valid_dataset, test_dataset, valid_labels, test_labels = train_test_split(test_dataset, test_labels, test_size=.5)\n",
    "train_labels = (np.arange(num_labels) == train_labels[:,None]).astype(np.float32)\n",
    "valid_labels = (np.arange(num_labels) == valid_labels[:,None]).astype(np.float32)\n",
    "test_labels = (np.arange(num_labels) == test_labels[:,None]).astype(np.float32)\n",
    "train_labels = train_labels.reshape((train_labels.shape[0], train_labels.shape[2]))\n",
    "valid_labels = valid_labels.reshape((valid_labels.shape[0], valid_labels.shape[2]))\n",
    "test_labels = test_labels.reshape((test_labels.shape[0], test_labels.shape[2]))\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "('Minibatch loss at step', 0, ':', 1.1226338)\n",
      "Minibatch accuracy: 66.4%\n",
      "Validation accuracy: 71.3%\n",
      "('Minibatch loss at step', 500, ':', 0.13852669)\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 95.2%\n",
      "('Minibatch loss at step', 1000, ':', 0.21013364)\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 95.7%\n",
      "('Minibatch loss at step', 1500, ':', 0.12611952)\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 95.9%\n",
      "('Minibatch loss at step', 2000, ':', 0.037048079)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 97.1%\n",
      "('Minibatch loss at step', 2500, ':', 0.088235386)\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 97.3%\n",
      "('Minibatch loss at step', 3000, ':', 0.088433363)\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 97.2%\n",
      "('Minibatch loss at step', 3500, ':', 0.082487665)\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 97.0%\n",
      "('Minibatch loss at step', 4000, ':', 0.019862831)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.7%\n",
      "('Minibatch loss at step', 4500, ':', 0.05177163)\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 97.6%\n",
      "('Minibatch loss at step', 5000, ':', 0.024548385)\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 98.3%\n",
      "('Minibatch loss at step', 5500, ':', 0.046790488)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 97.9%\n",
      "('Minibatch loss at step', 6000, ':', 0.050120175)\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 98.4%\n",
      "('Minibatch loss at step', 6500, ':', 0.059645701)\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 98.2%\n",
      "('Minibatch loss at step', 7000, ':', 0.039185777)\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 98.4%\n",
      "('Minibatch loss at step', 7500, ':', 0.056767587)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.2%\n",
      "('Minibatch loss at step', 8000, ':', 0.076769508)\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 98.3%\n",
      "('Minibatch loss at step', 8500, ':', 0.045621432)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 97.9%\n",
      "('Minibatch loss at step', 9000, ':', 0.057856992)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.5%\n",
      "('Minibatch loss at step', 9500, ':', 0.14799902)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.2%\n",
      "('Minibatch loss at step', 10000, ':', 0.028574616)\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 98.7%\n",
      "('Minibatch loss at step', 10500, ':', 0.041291971)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.0%\n",
      "('Minibatch loss at step', 11000, ':', 0.049690682)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.8%\n",
      "('Minibatch loss at step', 11500, ':', 0.043000467)\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 98.7%\n",
      "('Minibatch loss at step', 12000, ':', 0.033212457)\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 98.4%\n",
      "('Minibatch loss at step', 12500, ':', 0.060319059)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.4%\n",
      "('Minibatch loss at step', 13000, ':', 0.044539414)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.6%\n",
      "('Minibatch loss at step', 13500, ':', 0.041511454)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.2%\n",
      "('Minibatch loss at step', 14000, ':', 0.07966958)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.5%\n",
      "('Minibatch loss at step', 14500, ':', 0.034987107)\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 98.8%\n",
      "('Minibatch loss at step', 15000, ':', 0.057478063)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.7%\n",
      "('Minibatch loss at step', 15500, ':', 0.011763074)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.9%\n",
      "('Minibatch loss at step', 16000, ':', 0.038190387)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.5%\n",
      "('Minibatch loss at step', 16500, ':', 0.05494123)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.6%\n",
      "('Minibatch loss at step', 17000, ':', 0.050195843)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.8%\n",
      "('Minibatch loss at step', 17500, ':', 0.010452239)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.6%\n",
      "('Minibatch loss at step', 18000, ':', 0.058721371)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.9%\n",
      "('Minibatch loss at step', 18500, ':', 0.0043610162)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.9%\n",
      "('Minibatch loss at step', 19000, ':', 0.020041663)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.9%\n",
      "('Minibatch loss at step', 19500, ':', 0.02413376)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.4%\n",
      "('Minibatch loss at step', 20000, ':', 0.048886377)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.8%\n",
      "('Minibatch loss at step', 20500, ':', 0.052676938)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.8%\n",
      "('Minibatch loss at step', 21000, ':', 0.013356235)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.0%\n",
      "('Minibatch loss at step', 21500, ':', 0.046213254)\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 98.7%\n",
      "('Minibatch loss at step', 22000, ':', 0.089704365)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.0%\n",
      "('Minibatch loss at step', 22500, ':', 0.014861144)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.7%\n",
      "('Minibatch loss at step', 23000, ':', 0.023269257)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.1%\n",
      "('Minibatch loss at step', 23500, ':', 0.042892881)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.7%\n",
      "('Minibatch loss at step', 24000, ':', 0.025632858)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.1%\n",
      "('Minibatch loss at step', 24500, ':', 0.031347286)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.1%\n",
      "('Minibatch loss at step', 25000, ':', 0.057302609)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.1%\n",
      "('Minibatch loss at step', 25500, ':', 0.023979742)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.9%\n",
      "('Minibatch loss at step', 26000, ':', 0.051866747)\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 99.0%\n",
      "('Minibatch loss at step', 26500, ':', 0.016042616)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.1%\n",
      "('Minibatch loss at step', 27000, ':', 0.055624716)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.9%\n",
      "('Minibatch loss at step', 27500, ':', 0.058283344)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.0%\n",
      "('Minibatch loss at step', 28000, ':', 0.041962977)\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 99.0%\n",
      "('Minibatch loss at step', 28500, ':', 0.055062268)\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 98.9%\n",
      "('Minibatch loss at step', 29000, ':', 0.010430513)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.9%\n",
      "('Minibatch loss at step', 29500, ':', 0.025026981)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.0%\n",
      "('Minibatch loss at step', 30000, ':', 0.015296696)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.1%\n",
      "('Minibatch loss at step', 30500, ':', 0.061975576)\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 99.2%\n",
      "('Minibatch loss at step', 31000, ':', 0.02260711)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.0%\n",
      "('Minibatch loss at step', 31500, ':', 0.06218667)\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 99.2%\n",
      "('Minibatch loss at step', 32000, ':', 0.022376308)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.1%\n",
      "('Minibatch loss at step', 32500, ':', 0.0098197926)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.2%\n",
      "('Minibatch loss at step', 33000, ':', 0.041782677)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.1%\n",
      "('Minibatch loss at step', 33500, ':', 0.0095812306)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.1%\n",
      "('Minibatch loss at step', 34000, ':', 0.032307982)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.1%\n",
      "('Minibatch loss at step', 34500, ':', 0.061727539)\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 99.2%\n",
      "('Minibatch loss at step', 35000, ':', 0.01116008)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.4%\n",
      "('Minibatch loss at step', 35500, ':', 0.021191787)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.3%\n",
      "('Minibatch loss at step', 36000, ':', 0.050188489)\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 99.2%\n",
      "('Minibatch loss at step', 36500, ':', 0.031364325)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 98.9%\n",
      "('Minibatch loss at step', 37000, ':', 0.017479824)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.3%\n",
      "('Minibatch loss at step', 37500, ':', 0.011241921)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.3%\n",
      "('Minibatch loss at step', 38000, ':', 0.011490282)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.3%\n",
      "('Minibatch loss at step', 38500, ':', 0.0063703493)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.3%\n",
      "('Minibatch loss at step', 39000, ':', 0.055306613)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.2%\n",
      "('Minibatch loss at step', 39500, ':', 0.015778648)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.3%\n",
      "('Minibatch loss at step', 40000, ':', 0.015520494)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.2%\n",
      "('Minibatch loss at step', 40500, ':', 0.016497277)\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 99.2%\n",
      "('Minibatch loss at step', 41000, ':', 0.051723994)\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 99.3%\n",
      "('Minibatch loss at step', 41500, ':', 0.0065639778)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.3%\n",
      "('Minibatch loss at step', 42000, ':', 0.026182014)\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 99.3%\n",
      "('Minibatch loss at step', 42500, ':', 0.001398074)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.4%\n",
      "('Minibatch loss at step', 43000, ':', 0.032267913)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.3%\n",
      "('Minibatch loss at step', 43500, ':', 0.02006623)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.4%\n",
      "('Minibatch loss at step', 44000, ':', 0.028362969)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.3%\n",
      "('Minibatch loss at step', 44500, ':', 0.039475217)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.5%\n",
      "('Minibatch loss at step', 45000, ':', 0.044681985)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.2%\n",
      "('Minibatch loss at step', 45500, ':', 0.00081978412)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.4%\n",
      "('Minibatch loss at step', 46000, ':', 0.015323073)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.4%\n",
      "('Minibatch loss at step', 46500, ':', 0.015984436)\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 99.4%\n",
      "('Minibatch loss at step', 47000, ':', 0.031569786)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.3%\n",
      "('Minibatch loss at step', 47500, ':', 0.064301714)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.3%\n",
      "('Minibatch loss at step', 48000, ':', 0.008647575)\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 99.3%\n",
      "('Minibatch loss at step', 48500, ':', 0.0058899745)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.4%\n",
      "('Minibatch loss at step', 49000, ':', 0.048660487)\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 99.4%\n",
      "('Minibatch loss at step', 49500, ':', 0.0044234525)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.3%\n",
      "('Minibatch loss at step', 50000, ':', 0.03895393)\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 99.4%\n",
      "('Minibatch loss at step', 50500, ':', 0.0010213929)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.4%\n",
      "('Minibatch loss at step', 51000, ':', 0.023972945)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.4%\n",
      "('Minibatch loss at step', 51500, ':', 0.01132787)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.4%\n",
      "('Minibatch loss at step', 52000, ':', 0.0044867201)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.5%\n",
      "('Minibatch loss at step', 52500, ':', 0.028401533)\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 99.3%\n",
      "('Minibatch loss at step', 53000, ':', 0.0030615279)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.4%\n",
      "('Minibatch loss at step', 53500, ':', 0.011540522)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.4%\n",
      "('Minibatch loss at step', 54000, ':', 0.012908364)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.5%\n",
      "('Minibatch loss at step', 54500, ':', 0.019042302)\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 99.4%\n",
      "('Minibatch loss at step', 55000, ':', 0.024296073)\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 99.3%\n",
      "('Minibatch loss at step', 55500, ':', 0.027113937)\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 99.3%\n",
      "('Minibatch loss at step', 56000, ':', 0.014775371)\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 99.5%\n",
      "('Minibatch loss at step', 56500, ':', 0.020415569)\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 99.4%\n",
      "('Minibatch loss at step', 57000, ':', 0.028236011)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.4%\n",
      "('Minibatch loss at step', 57500, ':', 0.0017225856)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.4%\n",
      "('Minibatch loss at step', 58000, ':', 0.025979627)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.4%\n",
      "('Minibatch loss at step', 58500, ':', 0.032571752)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.5%\n",
      "('Minibatch loss at step', 59000, ':', 0.018600231)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.5%\n",
      "('Minibatch loss at step', 59500, ':', 0.014786465)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.3%\n",
      "('Minibatch loss at step', 60000, ':', 0.0053992039)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.5%\n",
      "('Minibatch loss at step', 60500, ':', 0.0069379616)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.4%\n",
      "('Minibatch loss at step', 61000, ':', 0.013933146)\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 99.5%\n",
      "('Minibatch loss at step', 61500, ':', 0.0094638448)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.4%\n",
      "('Minibatch loss at step', 62000, ':', 0.0059774527)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.5%\n",
      "('Minibatch loss at step', 62500, ':', 0.016725693)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.5%\n",
      "('Minibatch loss at step', 63000, ':', 0.046959929)\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 99.5%\n",
      "('Minibatch loss at step', 63500, ':', 0.0037316682)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.5%\n",
      "('Minibatch loss at step', 64000, ':', 0.026944561)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.5%\n",
      "('Minibatch loss at step', 64500, ':', 0.038433395)\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 99.5%\n",
      "('Minibatch loss at step', 65000, ':', 0.015690446)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.5%\n",
      "('Minibatch loss at step', 65500, ':', 0.039442807)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.5%\n",
      "('Minibatch loss at step', 66000, ':', 0.019199019)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.3%\n",
      "('Minibatch loss at step', 66500, ':', 0.020821249)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.6%\n",
      "('Minibatch loss at step', 67000, ':', 0.0051120911)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.6%\n",
      "('Minibatch loss at step', 67500, ':', 0.011464529)\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 99.6%\n",
      "('Minibatch loss at step', 68000, ':', 0.0084806103)\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 99.5%\n",
      "('Minibatch loss at step', 68500, ':', 0.0063888677)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.5%\n",
      "('Minibatch loss at step', 69000, ':', 0.0016404127)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.5%\n",
      "('Minibatch loss at step', 69500, ':', 0.019892609)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.5%\n",
      "('Minibatch loss at step', 70000, ':', 0.0044064024)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.6%\n",
      "('Minibatch loss at step', 70500, ':', 0.012026512)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.6%\n",
      "('Minibatch loss at step', 71000, ':', 0.003733315)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.5%\n",
      "('Minibatch loss at step', 71500, ':', 0.0047850348)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.5%\n",
      "('Minibatch loss at step', 72000, ':', 0.0019840745)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.5%\n",
      "('Minibatch loss at step', 72500, ':', 0.0022683567)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.6%\n",
      "('Minibatch loss at step', 73000, ':', 0.0050747013)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.6%\n",
      "('Minibatch loss at step', 73500, ':', 0.012576463)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.6%\n",
      "('Minibatch loss at step', 74000, ':', 0.0052124816)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.5%\n",
      "('Minibatch loss at step', 74500, ':', 0.011335772)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.6%\n",
      "('Minibatch loss at step', 75000, ':', 0.015096951)\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 99.6%\n",
      "('Minibatch loss at step', 75500, ':', 0.012304962)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.6%\n",
      "('Minibatch loss at step', 76000, ':', 0.010900819)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.6%\n",
      "('Minibatch loss at step', 76500, ':', 0.011924146)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.6%\n",
      "('Minibatch loss at step', 77000, ':', 0.00476901)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.5%\n",
      "('Minibatch loss at step', 77500, ':', 0.0091514699)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.6%\n",
      "('Minibatch loss at step', 78000, ':', 0.0083825821)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.6%\n",
      "('Minibatch loss at step', 78500, ':', 0.0032340989)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.6%\n",
      "('Minibatch loss at step', 79000, ':', 0.01227612)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.6%\n",
      "('Minibatch loss at step', 79500, ':', 0.014487403)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.5%\n",
      "('Minibatch loss at step', 80000, ':', 0.0050531547)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.6%\n",
      "('Minibatch loss at step', 80500, ':', 0.034706265)\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 99.6%\n",
      "('Minibatch loss at step', 81000, ':', 0.016421223)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.6%\n",
      "('Minibatch loss at step', 81500, ':', 0.067445099)\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 99.6%\n",
      "('Minibatch loss at step', 82000, ':', 0.012101329)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.6%\n",
      "('Minibatch loss at step', 82500, ':', 0.017728997)\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 99.5%\n",
      "('Minibatch loss at step', 83000, ':', 0.0060296273)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.5%\n",
      "('Minibatch loss at step', 83500, ':', 0.04156227)\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 99.6%\n",
      "('Minibatch loss at step', 84000, ':', 0.018068377)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.6%\n",
      "('Minibatch loss at step', 84500, ':', 0.0080643017)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.6%\n",
      "('Minibatch loss at step', 85000, ':', 0.010456678)\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 99.6%\n",
      "('Minibatch loss at step', 85500, ':', 0.0056560878)\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 99.6%\n",
      "('Minibatch loss at step', 86000, ':', 0.0048855804)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.6%\n",
      "('Minibatch loss at step', 86500, ':', 0.0096528763)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.7%\n",
      "('Minibatch loss at step', 87000, ':', 0.020387456)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.6%\n",
      "('Minibatch loss at step', 87500, ':', 0.010087797)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.6%\n",
      "('Minibatch loss at step', 88000, ':', 0.0021545445)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.7%\n",
      "('Minibatch loss at step', 88500, ':', 0.00039589906)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.6%\n",
      "('Minibatch loss at step', 89000, ':', 0.024502978)\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 99.6%\n",
      "('Minibatch loss at step', 89500, ':', 0.023757717)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.6%\n",
      "('Minibatch loss at step', 90000, ':', 0.00457589)\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 99.6%\n",
      "('Minibatch loss at step', 90500, ':', 0.0014928142)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.6%\n",
      "('Minibatch loss at step', 91000, ':', 0.0059224186)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.6%\n",
      "('Minibatch loss at step', 91500, ':', 0.0090606771)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.6%\n",
      "('Minibatch loss at step', 92000, ':', 0.0048297173)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.6%\n",
      "('Minibatch loss at step', 92500, ':', 0.0049198819)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.7%\n",
      "('Minibatch loss at step', 93000, ':', 0.014841095)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.6%\n",
      "('Minibatch loss at step', 93500, ':', 0.0088961385)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.6%\n",
      "('Minibatch loss at step', 94000, ':', 0.016297739)\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 99.7%\n",
      "('Minibatch loss at step', 94500, ':', 0.011245707)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.6%\n",
      "('Minibatch loss at step', 95000, ':', 0.006856679)\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 99.6%\n",
      "Test accuracy: 99.6%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "hidden_layer1_size = 1024\n",
    "hidden_layer2_size = 305\n",
    "hidden_lastlayer_size = 75\n",
    "data_size = 6\n",
    "\n",
    "use_multilayers = True\n",
    "\n",
    "regularization_meta=0.03\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, data_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "  weights_layer1 = tf.Variable(\n",
    "    tf.truncated_normal([data_size, hidden_layer1_size], stddev=0.0517))\n",
    "  biases_layer1 = tf.Variable(tf.zeros([hidden_layer1_size]))\n",
    "\n",
    "  if use_multilayers:\n",
    "    weights_layer2 = tf.Variable(\n",
    "      tf.truncated_normal([hidden_layer1_size, hidden_layer1_size], stddev=0.0441))\n",
    "    biases_layer2 = tf.Variable(tf.zeros([hidden_layer1_size]))\n",
    "\n",
    "    weights_layer3 = tf.Variable(\n",
    "      tf.truncated_normal([hidden_layer1_size, hidden_layer2_size], stddev=0.0441))\n",
    "    biases_layer3 = tf.Variable(tf.zeros([hidden_layer2_size]))\n",
    "    \n",
    "    weights_layer4 = tf.Variable(\n",
    "      tf.truncated_normal([hidden_layer2_size, hidden_lastlayer_size], stddev=0.0809))\n",
    "    biases_layer4 = tf.Variable(tf.zeros([hidden_lastlayer_size]))\n",
    "\n",
    "\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([hidden_lastlayer_size if use_multilayers else hidden_layer1_size, num_labels], stddev=0.1632))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "    \n",
    "  # get the NN models\n",
    "  def getNN4Layer(dSet, use_dropout):\n",
    "    input_to_layer1 = tf.matmul(dSet, weights_layer1) + biases_layer1\n",
    "    hidden_layer1_output = tf.nn.relu(input_to_layer1)\n",
    "    \n",
    "    \n",
    "    logits_hidden1 = None\n",
    "    if use_dropout:\n",
    "       dropout_hidden1 = tf.nn.dropout(hidden_layer1_output, keep_prob)\n",
    "       logits_hidden1 = tf.matmul(dropout_hidden1, weights_layer2) + biases_layer2\n",
    "    else:\n",
    "      logits_hidden1 = tf.matmul(hidden_layer1_output, weights_layer2) + biases_layer2\n",
    "    \n",
    "    hidden_layer2_output = tf.nn.relu(logits_hidden1)\n",
    "    \n",
    "    logits_hidden2 = None\n",
    "    if use_dropout:\n",
    "       dropout_hidden2 = tf.nn.dropout(hidden_layer2_output, keep_prob)\n",
    "       logits_hidden2 = tf.matmul(dropout_hidden2, weights_layer3) + biases_layer3\n",
    "    else:\n",
    "      logits_hidden2 = tf.matmul(hidden_layer2_output, weights_layer3) + biases_layer3\n",
    "    \n",
    "    \n",
    "    hidden_layer3_output = tf.nn.relu(logits_hidden2)\n",
    "    logits_hidden3 = None\n",
    "    if use_dropout:\n",
    "       dropout_hidden3 = tf.nn.dropout(hidden_layer3_output, keep_prob)\n",
    "       logits_hidden3 = tf.matmul(dropout_hidden3, weights_layer4) + biases_layer4\n",
    "    else:\n",
    "      logits_hidden3 = tf.matmul(hidden_layer3_output, weights_layer4) + biases_layer4\n",
    "    \n",
    "    \n",
    "    hidden_layer4_output = tf.nn.relu(logits_hidden3)\n",
    "    logits = None\n",
    "    if use_dropout:\n",
    "       dropout_hidden4 = tf.nn.dropout(hidden_layer4_output, keep_prob)\n",
    "       logits = tf.matmul(dropout_hidden4, weights) + biases\n",
    "    else:\n",
    "      logits = tf.matmul(hidden_layer4_output, weights) + biases\n",
    "    \n",
    "    return logits\n",
    "\n",
    "  # get the NN models\n",
    "  def getNN1Layer(dSet, use_dropout, w1, b1, w, b):\n",
    "    input_to_layer1 = tf.matmul(dSet, w1) + b1\n",
    "    hidden_layer1_output = tf.nn.relu(input_to_layer1)\n",
    "        \n",
    "    logits = None\n",
    "    if use_dropout:\n",
    "       dropout_hidden1 = tf.nn.dropout(hidden_layer1_output, keep_prob)\n",
    "       logits = tf.matmul(dropout_hidden1, w) + b\n",
    "    else:\n",
    "      logits = tf.matmul(hidden_layer1_output, w) + b\n",
    "    \n",
    "    return logits\n",
    "\n",
    "  \n",
    "  \n",
    "  # Training computation.\n",
    "  logits = getNN4Layer(tf_train_dataset, True)  \n",
    "  logits_valid = getNN4Layer(tf_valid_dataset, False)\n",
    "  logits_test = getNN4Layer(tf_test_dataset, False)\n",
    "    \n",
    "  \n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  #loss_l2 = loss + (regularization_meta * (tf.nn.l2_loss(weights)))\n",
    "  \n",
    "  global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "  learning_rate = tf.train.exponential_decay(0.3, global_step, 3500, 0.86, staircase=True)\n",
    "  \n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(logits_valid)\n",
    "  test_prediction = tf.nn.softmax(logits_test)\n",
    "\n",
    "\n",
    "\n",
    "num_steps = 95001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in xrange(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    \n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    \n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, keep_prob:0.75}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step\", step, \":\", l)\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(train_prediction.eval(feed_dict={tf_train_dataset : batch_data, tf_train_labels : batch_labels, keep_prob:1.0}), batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(feed_dict={keep_prob:1.0}), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(feed_dict={keep_prob:1.0}), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = data.ix[:, ['theta1', 'theta2', 'theta3', 'rate1', 'rate2', 'rate3']].values\n",
    "y = data.ix[:, 'level'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
    "y_train[0:10]\n",
    "stdsc = StandardScaler()\n",
    "X_train_norm = stdsc.fit_transform(X_train)\n",
    "X_test_norm = stdsc.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math, functools\n",
    "p = [20, 10, 30]\n",
    "functools.reduce(math.gcd, p)\n",
    "\n",
    "bound = np.array([[80], [40], [30]])\n",
    "u = np.array([[100], [40], [30]])\n",
    "np.minimum(bound, u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('theta1', 0.48629832630424419)\n",
      "('theta2', 0.14562975570584419)\n",
      "('theta3', 0.062519674401128489)\n",
      "('rate1', 0.17708399655037799)\n",
      "('rate2', 0.082285571954176212)\n",
      "('rate3', 0.046182675084229061)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.98303732084575002, 0.95321583652618136)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest = RandomForestClassifier(n_estimators = 2, n_jobs = -1, class_weight='balanced')\n",
    "forest.fit(X_train, y_train)\n",
    "importances = forest.feature_importances_\n",
    "cols = data.columns\n",
    "importances = zip(cols, importances)\n",
    "for name, val in importances:\n",
    "    print(name, val)\n",
    "\n",
    "forest.score(X_train, y_train), forest.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 116 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 2.]), 2.0)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "forest.predict(X_test[0, :].reshape(1, -1)), y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pickle.dump(forest, open(os.path.join('./', 'subspace_clf.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# clf = pickle.load(open(os.path.join('./', 'subspace_clf.pkl'), 'rb'))\n",
    "forest.predict(np.array([[0,0,0,0,0,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = data.ix[:, ['theta1', 'theta2', 'theta3', 'rate1', 'rate2', 'rate3']].values\n",
    "y = data.ix[:, 'level'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
    "y_train[0:10]\n",
    "stdsc = StandardScaler()\n",
    "X_train_norm = stdsc.fit_transform(X_train)\n",
    "X_test_norm = stdsc.transform(X_test)\n",
    "\n",
    "forest = KNeighborsClassifier(n_neighbors=  10, n_jobs = -1)\n",
    "forest.fit(X_train, y_train)\n",
    "importances = forest.feature_importances_\n",
    "cols = data.columns\n",
    "importances = zip(cols, importances)\n",
    "for name, val in importances:\n",
    "    print(name, val)\n",
    "\n",
    "forest.score(X_train, y_train), forest.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta1 0.346634028685\n",
      "theta2 0.133626355006\n",
      "theta3 0.0645949198916\n",
      "rate1 0.289684103189\n",
      "rate2 0.107899096466\n",
      "rate3 0.0575614967627\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.99926864326465825, 0.96944454997910412)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "X = data.ix[:, ['theta1', 'theta2', 'theta3', 'rate1', 'rate2', 'rate3']].values\n",
    "y = data.ix[:, 'level'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion = 'gini', max_depth = None, class_weight = 'balanced')\n",
    "clf.fit(X_train, y_train)\n",
    "importances = clf.feature_importances_\n",
    "cols = data.columns\n",
    "importances = zip(cols, importances)\n",
    "for name, val in importances:\n",
    "    print(name, val)\n",
    "\n",
    "clf.score(X_train, y_train), clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(clf, open(os.path.join('./Cyber/', 'subspace_clf_decision_tree.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.95723116219667947)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "X = data.ix[:, ['theta1', 'theta2', 'theta3', 'rate1', 'rate2', 'rate3']].values\n",
    "y_orig = data.ix[:, 'level'].values\n",
    "y = label_binarize(y_orig, classes=[0, 1, 2])\n",
    "n_classes = y.shape[1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
    "\n",
    "stdsc = StandardScaler()\n",
    "X_train_norm = stdsc.fit_transform(X_train)\n",
    "X_test_norm = stdsc.transform(X_test)\n",
    "\n",
    "clf = OneVsRestClassifier(DecisionTreeClassifier(criterion = 'gini', max_depth = None, class_weight = 'balanced'))\n",
    "clf.fit(X_train, y_train)\n",
    "# importances = clf.feature_importances_\n",
    "# cols = data.columns\n",
    "# importances = zip(cols, importances)\n",
    "# for name, val in importances:\n",
    "#     print(name, val)\n",
    "\n",
    "clf.score(X_train, y_train), clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.14730013,  0.95743787,  1.        ]),\n",
       " array([ 1.        ,  0.94867171,  0.        ]))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEPCAYAAAC+35gCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3CxpGGOMidv/D1SBvLbnE0MyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7faabadcf7d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_score = clf.predict_proba(X_test)\n",
    "\n",
    "\n",
    "# Compute Precision-Recall and plot curve\n",
    "n_classes = 3\n",
    "prcsn = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "thrshld = {}\n",
    "for i in range(n_classes):\n",
    "    prcsn[i], recall[i], thrshld[i] = precision_recall_curve(y_test[:, i],y_score[:, i])\n",
    "    average_precision[i] = average_precision_score(y_test[:, i], y_score[:, i])\n",
    "\n",
    "# min_prcsn1 = 0.95  # let's do it for s1\n",
    "# min_thrshld1=min([thrshld[0][i] for i in range(len(thrshld[0])) if prcsn[0][i]>=min_prcsn1])\n",
    "\n",
    "# min_prcsn2 = 0.95  # let's do it for s2\n",
    "# min_thrshld2=min([thrshld[1][i] for i in range(len(thrshld[1])) if prcsn[1][i]>=min_prcsn2])\n",
    "\n",
    "# y_pred_adjusted=[1 if y_s[0]>min_thrshld1 else 2 if y_s[1] > min_thrshld2 else 3 for y_s in y_score]\n",
    "# y_pred_adjusted = label_binarize(y_pred_adjusted, classes=[1, 2, 3])\n",
    "# # y_pred_adjusted\n",
    "# # np.array_equal(y_pred_adjusted, y_test)\n",
    "# print('accuracy of adjusted y pred: ', \n",
    "#       sum([np.array_equal(y_pred_adjusted[i], y_test[i]) for i in np.arange(len(y_pred_adjusted))]) / len(y_pred_adjusted))\n",
    "# # len(y_pred_adjusted), len(y_test), \n",
    "\n",
    "# Plot Precision-Recall curve for each class\n",
    "\n",
    "plt.clf()\n",
    "for i in range(n_classes):\n",
    "    plt.plot(recall[i], prcsn[i], linewidth = 6 - 2*i,\n",
    "             label='Precision-recall curve of fault tolerance level {0}'\n",
    "                   ''.format(i+1))\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall', fontsize= 15)\n",
    "plt.ylabel('Precision', fontsize= 15)\n",
    "\n",
    "# plt.title('Extension of Precision-Recall curve to multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 1.39 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0, 0, 1]]), array([0, 0, 1]))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "clf.predict(X_test[999, :].reshape(1, -1)), y_test[999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "X = data.ix[:, ['theta1', 'theta2', 'theta3', 'rate1', 'rate2', 'rate3']].values\n",
    "y = data.ix[:, 'level'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
    "y_train[0:10]\n",
    "stdsc = StandardScaler()\n",
    "X_train_norm = stdsc.fit_transform(X_train)\n",
    "X_test_norm = stdsc.transform(X_test)\n",
    "\n",
    "clf = SVC(C = 1., class_weight = 'balanced')\n",
    "clf.fit(X_train[0:9001, :], y_train[0:9001])\n",
    "# importances = clf.feature_importances_\n",
    "# cols = data.columns\n",
    "# importances = zip(cols, importances)\n",
    "# for name, val in importances:\n",
    "#     print(name, val)\n",
    "\n",
    "clf.score(X_train_norm, y_train), clf.score(X_test_norm, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "clf.score(X_train_norm, y_train), clf.score(X_test_norm, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7144101035901802, 0.71540229885057471)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data.ix[:, ['theta1', 'theta2', 'theta3', 'rate1', 'rate2', 'rate3']].values\n",
    "y = data.ix[:, 'level'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
    "y_train[0:10]\n",
    "stdsc = StandardScaler()\n",
    "X_train_norm = stdsc.fit_transform(X_train)\n",
    "X_test_norm = stdsc.transform(X_test)\n",
    "\n",
    "clf = SGDClassifier(n_jobs = -1, class_weight= 'balanced', penalty = 'l2', average = True, loss=\"log\", \n",
    "                    alpha=1., n_iter=2, fit_intercept=True)  \n",
    "clf.fit(X_train_norm, y_train)\n",
    "clf.score(X_train_norm, y_train), clf.score(X_test_norm, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "880875"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 363 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 2.]), 2.0)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "clf.predict(X_test[0, :].reshape(1, -1)), y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "i = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962.828210115\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZkAAAEZCAYAAABFFVgWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7faab736f0d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "\n",
    "from sklearn.learning_curve import learning_curve\n",
    "\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and traning learning curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : integer, cross-validation generator, optional\n",
    "        If an integer is passed, it is the number of folds (defaults to 3).\n",
    "        Specific cross-validation objects can be passed, see\n",
    "        sklearn.cross_validation module for the list of possible objects\n",
    "\n",
    "    n_jobs : integer, optional\n",
    "        Number of jobs to run in parallel (default 1).\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "\n",
    "\n",
    "X = data.ix[:, ['theta1', 'theta2', 'theta3', 'rate1', 'rate2', 'rate3']].values\n",
    "y = data.ix[:, 'level'].values\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
    "\n",
    "# X = X_train[0:200000, :]\n",
    "# y = y_train[0:200000]\n",
    "\n",
    "\n",
    "title = \"Learning Curves (Decision Tree)\"\n",
    "# Cross validation with 100 iterations to get smoother mean test and train\n",
    "# score curves, each time with 20% data randomly selected as a validation set.\n",
    "cv = cross_validation.ShuffleSplit(X.shape[0], n_iter=100,\n",
    "                                   test_size=0.2, random_state=0)\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "estimator = DecisionTreeClassifier(class_weight = 'balanced')\n",
    "plot_learning_curve(estimator, title, X, y, ylim=(0.7, 1.01), cv=cv, n_jobs=4)\n",
    "print(time.time() - start)\n",
    "# title = \"Learning Curves (SVM, RBF kernel, $\\gamma=0.001$)\"\n",
    "# # SVC is more expensive so we do a lower number of CV iterations:\n",
    "# cv = cross_validation.ShuffleSplit(digits.data.shape[0], n_iter=10,\n",
    "#                                    test_size=0.2, random_state=0)\n",
    "# estimator = SVC(gamma=0.001)\n",
    "# plot_learning_curve(estimator, title, X, y, (0.7, 1.01), cv=cv, n_jobs=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
